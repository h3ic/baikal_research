{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words(\"russian\")\n",
    "stopwords_list.extend(stopwords.words(\"english\"))\n",
    "import pymorphy2\n",
    "\n",
    "analyzer = pymorphy2.MorphAnalyzer()\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "with open('bios_corpus.json', 'r') as f:\n",
    "    corpus = json.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "data": {
      "text/plain": "1604"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "emoji_pattern = re.compile('['\n",
    "                           u'\\U0001F600-\\U0001F64F'\n",
    "                           u'\\U0001F300-\\U0001F5FF'\n",
    "                           u'\\U0001F680-\\U0001F6FF'\n",
    "                           u'\\U0001F1E0-\\U0001F1FF'\n",
    "                           u'\\U0001F1F2-\\U0001F1F4'\n",
    "                           u'\\U0001F1E6-\\U0001F1FF'\n",
    "                           u'\\U0001F600-\\U0001F64F'\n",
    "                           u'\\U00002702-\\U000027B0'\n",
    "                           u'\\U000024C2-\\U0001F251'\n",
    "                           u'\\U0001f926-\\U0001f937'\n",
    "                           u'\\U0001F1F2'\n",
    "                           u'\\U0001F1F4'\n",
    "                           u'\\U0001F620'\n",
    "                           u'\\u200d'\n",
    "                           u'\\u2640-\\u2642'\n",
    "                           u'\\U0001F919'  # added emojis\n",
    "                           u'\\U000026E9'\n",
    "                           u'\\U0001F913'\n",
    "                           u'\\U0001F9D0'\n",
    "                           u'\\U00002705'\n",
    "                           u'\\U00002753'\n",
    "                           u'\\U0001F926'\n",
    "                           u'\\U0001F937'\n",
    "                           u'\\U0001F918'\n",
    "                           u'\\U0001F9ED'\n",
    "                           ']+', flags=re.UNICODE)\n",
    "\n",
    "url_pattern = re.compile(r'https?://(?:www\\.)?(?:[-a-zA-Z0-9@:%._\\+~#=/?&]+)')\n",
    "brackets_id_pattern = re.compile(r'\\[\\w+\\|\\w+\\-?\\w+?\\s?\\w+\\-?\\w+?\\]')\n",
    "tag_patten = re.compile(r'@\\w+')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "def regex(corpus_json):\n",
    "    edited_corpus = []\n",
    "    for i, doc in enumerate(corpus_json):\n",
    "        edited_doc = re.sub('\\n', ' ', corpus_json[i])\n",
    "        edited_doc = emoji_pattern.sub('', edited_doc)\n",
    "        edited_doc = url_pattern.sub('', edited_doc)\n",
    "        edited_doc = brackets_id_pattern.sub('', edited_doc)\n",
    "        edited_doc = tag_patten.sub('', edited_doc)\n",
    "        edited_corpus.append(edited_doc)\n",
    "\n",
    "\n",
    "def preprocess(doc: str):\n",
    "    re_tokenizer = RegexpTokenizer(r'[А-яA-z-ё]+')\n",
    "    tokens = re_tokenizer.tokenize(doc)\n",
    "    tokens = [token.lower() for token in tokens if len(token) > 1 and token.isalpha()]\n",
    "    tokenized_doc = [word for word in tokens if word not in stopwords_list]\n",
    "    norm_doc = [analyzer.parse(word)[0].normal_form for word in tokenized_doc]\n",
    "    return norm_doc\n",
    "\n",
    "\n",
    "def prepare_ner(tokenized_corpus):\n",
    "    nouns_corpus = []\n",
    "    for doc in tokenized_corpus:\n",
    "        nouns_doc = [word for word in doc if analyzer.parse(word)[0].tag.POS == 'NOUN']\n",
    "        if nouns_doc:\n",
    "            nouns_corpus.append(nouns_doc)\n",
    "    return nouns_corpus"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "data": {
      "text/plain": "[['глава',\n  'местный',\n  'администрация',\n  'муниципальный',\n  'округ',\n  'васильевский',\n  'чгэк',\n  'спбп',\n  'пётр',\n  'великий',\n  'старший',\n  'преподаватель',\n  'спбгипср'],\n ['рыбак', 'охотник', 'стаж', 'любой', 'снаряжение', 'лс'],\n ['активный',\n  'пенсионер',\n  'официальный',\n  'партнёр',\n  'корпорация',\n  'сибирский',\n  'здоровье',\n  'директор',\n  'центр',\n  'обслуживание',\n  'клиент',\n  'коломна'],\n ['настоящий',\n  'швеция',\n  'её',\n  'человек',\n  'living',\n  'sweden',\n  'det',\n  'har',\n  'nt',\n  'riktigt',\n  'inspirerad',\n  'av',\n  'verkliga',\n  'ndelser',\n  'sverige',\n  'malm',\n  'sweden'],\n ['руководитель', 'бид', 'главный', 'редактор', 'дизайнер']]"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized = []\n",
    "for bio in corpus:\n",
    "    if bio != '':\n",
    "        tokenized_bio = preprocess(bio)\n",
    "        tokenized.append(tokenized_bio)\n",
    "\n",
    "tokenized[:5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "with open('tokenized_corpus.json', 'w') as f:\n",
    "    json.dump(tokenized, f, ensure_ascii=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "data": {
      "text/plain": "[['глава',\n  'администрация',\n  'округ',\n  'чгэк',\n  'спбп',\n  'пётр',\n  'старший',\n  'преподаватель',\n  'спбгипср'],\n ['рыбак', 'охотник', 'стаж', 'снаряжение'],\n ['пенсионер',\n  'партнёр',\n  'корпорация',\n  'здоровье',\n  'директор',\n  'центр',\n  'обслуживание',\n  'клиент',\n  'коломна'],\n ['швеция', 'человек'],\n ['руководитель', 'редактор', 'дизайнер']]"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nouns_corpus = prepare_ner(tokenized)\n",
    "\n",
    "nouns_corpus[:5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### most frequent nouns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1414\n"
     ]
    },
    {
     "data": {
      "text/plain": "[('здоровье', 79),\n ('жизнь', 79),\n ('байкал', 51),\n ('красота', 48),\n ('бизнес', 43),\n ('путешествие', 32),\n ('год', 32),\n ('мир', 31),\n ('мама', 29),\n ('человек', 23),\n ('семья', 22),\n ('зож', 21),\n ('ссылка', 20),\n ('иркутск', 20),\n ('директ', 19),\n ('природа', 19),\n ('эксперт', 18),\n ('консультация', 18),\n ('ребёнок', 18),\n ('доход', 17),\n ('продукт', 16),\n ('работа', 16),\n ('дом', 16),\n ('эко', 15),\n ('заказ', 15),\n ('партнёр', 14),\n ('любовь', 14),\n ('предприниматель', 14),\n ('вопрос', 13),\n ('тур', 13),\n ('спорт', 13),\n ('опыт', 12),\n ('страна', 12),\n ('команда', 12),\n ('экскурсия', 12),\n ('проект', 12),\n ('гид', 11),\n ('покупка', 11),\n ('город', 11),\n ('магазин', 11),\n ('отдых', 11),\n ('консультант', 11),\n ('москва', 11),\n ('программа', 10),\n ('развитие', 10),\n ('компания', 10),\n ('россия', 10),\n ('молодость', 9),\n ('сибирь', 9),\n ('секрет', 9)]"
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_KEYWORDS = 50\n",
    "flat_corpus = [word for doc in nouns_corpus for word in doc]\n",
    "counter = Counter(flat_corpus)\n",
    "counter_sorted = dict(sorted(counter.items(),\n",
    "                             key=lambda item: item[1], reverse=True))\n",
    "print(len(counter_sorted))\n",
    "list(counter_sorted.items())[:N_KEYWORDS]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### most frequent words"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3475\n"
     ]
    },
    {
     "data": {
      "text/plain": "[('здоровье', 79),\n ('жизнь', 79),\n ('байкал', 51),\n ('wellness', 51),\n ('siberian', 49),\n ('красота', 48),\n ('любить', 46),\n ('бизнес', 43),\n ('moscow', 33),\n ('путешествие', 32),\n ('год', 32),\n ('мир', 31),\n ('писать', 31),\n ('мама', 29),\n ('russia', 25),\n ('жить', 25),\n ('человек', 23),\n ('путешествовать', 22),\n ('ваш', 22),\n ('семья', 22),\n ('зож', 21),\n ('мой', 21),\n ('ссылка', 20),\n ('иркутск', 20),\n ('свой', 20),\n ('директ', 19),\n ('природа', 19),\n ('эксперт', 18),\n ('консультация', 18),\n ('ребёнок', 18),\n ('весь', 17),\n ('доход', 17),\n ('ru', 17),\n ('love', 17),\n ('делиться', 17),\n ('продукт', 16),\n ('whatsapp', 16),\n ('работа', 16),\n ('дом', 16),\n ('эко', 15),\n ('здоровый', 15),\n ('заказ', 15),\n ('счастливый', 15),\n ('партнёр', 14),\n ('сибирский', 14),\n ('работать', 14),\n ('любовь', 14),\n ('предприниматель', 14),\n ('вопрос', 13),\n ('тур', 13)]"
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_corpus = [word for doc in tokenized for word in doc]\n",
    "counter = Counter(flat_corpus)\n",
    "counter_sorted = dict(sorted(counter.items(),\n",
    "                             key=lambda item: item[1], reverse=True))\n",
    "print(len(counter_sorted))\n",
    "list(counter_sorted.items())[:N_KEYWORDS]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}